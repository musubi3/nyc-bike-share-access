{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653b4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from scipy.spatial import cKDTree\n",
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and sampling...\n",
      "Loaded 124475 rows.\n",
      "Saving 100000 rows to ../data/202501-citibike-sample.csv...\n",
      "Done! You have a perfect random sample.\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = '../data/202501-citibike-tripdata_3.csv'\n",
    "OUTPUT_FILE = '../data/202501-citibike-sample.csv'\n",
    "TARGET_ROWS = 100000\n",
    "\n",
    "print(\"Reading and sampling...\")\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows.\")\n",
    "\n",
    "if len(df) > TARGET_ROWS:\n",
    "    df = df.sample(n=TARGET_ROWS)\n",
    "\n",
    "print(f\"Saving {len(df)} rows to {OUTPUT_FILE}...\")\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"Done! You have a perfect random sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dddc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for...\n",
      "  - County 005: Found 361 tracts\n",
      "  - County 047: Found 805 tracts\n",
      "  - County 061: Found 310 tracts\n",
      "  - County 081: Found 725 tracts\n",
      "  - County 085: Found 126 tracts\n",
      "\n",
      "Success! Saved 'nyc_transit_equity.csv'\n"
     ]
    }
   ],
   "source": [
    "YEAR = \"2022\"\n",
    "DATASET = \"acs/acs5\"\n",
    "BASE_URL = f\"https://api.census.gov/data/{YEAR}/{DATASET}\"\n",
    "VARIABLES = \"NAME,B08201_001E,B08201_002E\"\n",
    "\n",
    "# NYC County Codes (FIPS): Bronx(005), Brooklyn(047), Manhattan(061), Queens(081), Staten Island(085)\n",
    "counties = [\"005\", \"047\", \"061\", \"081\", \"085\"]\n",
    "state_code = \"36\" # New York\n",
    "\n",
    "all_data = []\n",
    "\n",
    "print(\"Fetching data for...\")\n",
    "for county in counties:\n",
    "    url = f\"{BASE_URL}?get={VARIABLES}&for=tract:*&in=state:{state_code}&in=county:{county}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        headers = data[0]\n",
    "        rows = data[1:]\n",
    "        df_county = pd.DataFrame(rows, columns=headers)\n",
    "        all_data.append(df_county)\n",
    "        print(f\"  - County {county}: Found {len(rows)} tracts\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - Error fetching county {county}: {e}\")\n",
    "\n",
    "nyc_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "nyc_df = nyc_df.rename(columns={\n",
    "    \"B08201_001E\": \"total_households\",\n",
    "    \"B08201_002E\": \"no_vehicle_households\",\n",
    "    \"state\": \"state_fips\",\n",
    "    \"county\": \"county_fips\",\n",
    "    \"tract\": \"tract_fips\"\n",
    "})\n",
    "\n",
    "nyc_df[\"GEOID\"] = nyc_df[\"state_fips\"] + nyc_df[\"county_fips\"] + nyc_df[\"tract_fips\"]\n",
    "nyc_df[\"total_households\"] = pd.to_numeric(nyc_df[\"total_households\"])\n",
    "nyc_df[\"no_vehicle_households\"] = pd.to_numeric(nyc_df[\"no_vehicle_households\"])\n",
    "nyc_df[\"pct_no_vehicle\"] = nyc_df[\"no_vehicle_households\"] / nyc_df[\"total_households\"]\n",
    "\n",
    "nyc_df.to_csv(\"../data/nyc_transit_equity.csv\", index=False)\n",
    "print(\"\\nSuccess! Saved 'nyc_transit_equity.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51166235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GeoJSON from https://data.cityofnewyork.us/resource/63ge-mke6.geojson?$limit=5000...\n",
      "Success! Downloaded 2325 tracts.\n",
      "Saved to 'nyc_tracts.geojson'. Move this file to your data/ folder.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://data.cityofnewyork.us/resource/63ge-mke6.geojson?$limit=5000\"\n",
    "\n",
    "print(f\"Downloading GeoJSON from {url}...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    count = len(data.get('features', []))\n",
    "    print(f\"Success! Downloaded {count} tracts.\")\n",
    "    \n",
    "    if count > 0:\n",
    "        with open('../data/nyc_tracts.geojson', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print(\"Saved to 'nyc_tracts.geojson'. Move this file to your data/ folder.\")\n",
    "    else:\n",
    "        print(\"Error: The downloaded file has 0 features.\")\n",
    "else:\n",
    "    print(f\"Error: Failed to download. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0d6826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_21148\\868742574.py:19: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  df_tracts['centroid'] = df_tracts.geometry.centroid\n",
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_21148\\868742574.py:20: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tract_points = list(zip(df_tracts.centroid.x, df_tracts.centroid.y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out Manhattan from 1234 tracts...\n",
      "Remaining tracts: 1232\n",
      "Top Candidates (Outer Boroughs Only): ['Spring Creek-Starrett City', 'Parkchester', 'Brighton Beach', 'Flushing-Willets Point', 'Co-op City', 'Coney Island-Sea Gate', 'Norwood', 'Soundview-Bruckner-Bronx River', 'Rockaway Beach-Arverne-Edgemere', 'Brownsville']\n",
      "\n",
      "✅ Success! Saved to ../data/gap_data.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df_tracts = gpd.read_file('../data/nyc_tracts.geojson')\n",
    "df_equity = pd.read_csv('../data/nyc_transit_equity.csv')\n",
    "df_stations = pd.read_csv('../data/202501-citibike-sample.csv')\n",
    "\n",
    "W_DENSITY = 0.40\n",
    "W_NEED = 0.45\n",
    "W_GAP = 0.15\n",
    "\n",
    "if 'geoid' in df_tracts.columns: df_tracts = df_tracts.rename(columns={'geoid': 'GEOID'})\n",
    "if 'geoid' in df_equity.columns: df_equity = df_equity.rename(columns={'geoid': 'GEOID'})\n",
    "df_tracts['GEOID'] = df_tracts['GEOID'].astype(str).str.strip()\n",
    "df_equity['GEOID'] = df_equity['GEOID'].astype(str).str.strip()\n",
    "\n",
    "df_stations = df_stations.dropna(subset=['start_lat', 'start_lng'])\n",
    "station_points = list(zip(df_stations.start_lng, df_stations.start_lat))\n",
    "\n",
    "df_tracts = df_tracts[df_tracts.geometry.notna()]\n",
    "df_tracts['centroid'] = df_tracts.geometry.centroid\n",
    "tract_points = list(zip(df_tracts.centroid.x, df_tracts.centroid.y))\n",
    "\n",
    "tree = cKDTree(station_points)\n",
    "dist, idx = tree.query(tract_points, k=1)\n",
    "df_tracts['dist_to_station'] = dist\n",
    "\n",
    "df_merged = df_tracts.merge(df_equity, on='GEOID', how='inner')\n",
    "candidates = df_merged.copy()\n",
    "candidates = candidates[candidates['dist_to_station'] > 0.005]\n",
    "\n",
    "for col in candidates.columns:\n",
    "    if col.lower() in ['boroname', 'boro_name', 'borough']:\n",
    "        candidates = candidates.rename(columns={col: 'boroname'})\n",
    "        break\n",
    "\n",
    "if 'boroname' in candidates.columns:\n",
    "    print(f\"Filtering out Manhattan from {len(candidates)} tracts...\")\n",
    "    candidates = candidates[~candidates['boroname'].isin(['Manhattan', 'New York'])]\n",
    "    print(f\"Remaining tracts: {len(candidates)}\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: Could not find 'boroname' column. Manhattan was not filtered.\")\n",
    "\n",
    "max_density = candidates['total_households'].max()\n",
    "max_carfree = candidates['pct_no_vehicle'].max()\n",
    "max_gap = candidates['dist_to_station'].max()\n",
    "\n",
    "candidates['norm_density'] = candidates['total_households'] / max_density if max_density > 0 else 0\n",
    "candidates['norm_carfree'] = candidates['pct_no_vehicle'] / max_carfree if max_carfree > 0 else 0\n",
    "candidates['norm_gap'] = candidates['dist_to_station'] / max_gap if max_gap > 0 else 0\n",
    "\n",
    "candidates['Expansion_Score'] = (\n",
    "    (candidates['norm_density'] * W_DENSITY) + \n",
    "    (candidates['norm_carfree'] * W_NEED) + \n",
    "    (candidates['norm_gap'] * W_GAP)\n",
    ")\n",
    "\n",
    "nta_col = 'ntaname' if 'ntaname' in candidates.columns else 'NTAName'\n",
    "neighborhood_ranks = candidates.groupby(nta_col).agg({\n",
    "    'Expansion_Score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "top_candidates = neighborhood_ranks.sort_values('Expansion_Score', ascending=False).head(10)\n",
    "top_names = top_candidates[nta_col].tolist()\n",
    "print(f\"Top Candidates (Outer Boroughs Only): {top_names}\")\n",
    "\n",
    "all_neighborhoods = df_tracts.dissolve(by=nta_col).reset_index()\n",
    "candidate_geoms = all_neighborhoods[all_neighborhoods[nta_col].isin(top_names)].copy()\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(top_names)\n",
    "\n",
    "for i, row1 in candidate_geoms.iterrows():\n",
    "    for j, row2 in candidate_geoms.iterrows():\n",
    "        if i >= j: continue \n",
    "        if row1.geometry.buffer(0.0001).intersects(row2.geometry):\n",
    "            G.add_edge(row1[nta_col], row2[nta_col])\n",
    "\n",
    "mapping = {}\n",
    "display_groups = []\n",
    "\n",
    "for component in nx.connected_components(G):\n",
    "    members = list(component)\n",
    "    members.sort() \n",
    "    group_name = \" & \".join(members)\n",
    "    for member in members:\n",
    "        mapping[member] = group_name\n",
    "    display_groups.append(group_name)\n",
    "\n",
    "output_data = {\n",
    "    \"generated_at\": pd.Timestamp.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"target_zones\": top_names,\n",
    "    \"group_mapping\": mapping,\n",
    "    \"display_list\": display_groups\n",
    "}\n",
    "\n",
    "with open('../data/gap_data.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Success! Saved to ../data/gap_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
